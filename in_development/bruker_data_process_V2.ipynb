{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bruker raw ome-tiff preparation for preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sima and other dependents\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy import signal\n",
    "import h5py\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL.TiffTags import TAGS\n",
    "import tifffile as tiff\n",
    "from lxml.html.soupparser import fromstring\n",
    "from lxml.etree import tostring\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import bruker_marked_pts_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load tiff data and get data shape\n",
    "\n",
    "def uint16_scale(img):\n",
    "    tmp = img - np.min(img) # shift values such that there are no negatives\n",
    "\n",
    "    ratio = np.amax(tmp) / 65535.0\n",
    "\n",
    "    return np.squeeze(tmp/ratio) \n",
    "\n",
    "\n",
    "def read_shape_tiff(data_path):\n",
    "    \n",
    "    data = uint16_scale(tiff.imread(data_path)).astype('uint16')\n",
    "    data_shape = data.shape\n",
    "\n",
    "    return data, data_shape\n",
    "\n",
    "def get_ometif_xy_shape(fpath):\n",
    "    # read first tiff to get data shape\n",
    "    first_tif = tiff.imread(fpath, key=0, is_ome=True)\n",
    "    return first_tif.shape\n",
    "\n",
    "\n",
    "def get_tif_meta(tif_path):\n",
    "    meta_dict = {}\n",
    "    # iterate through metadata and create dict for key/value pairs\n",
    "    with Image.open(tif_path) as img:\n",
    "        for key in img.tag.iterkeys():\n",
    "            if key in TAGS:\n",
    "                meta_dict[TAGS[key]] = img.tag[key] \n",
    "            else:\n",
    "                meta_dict[key] = img.tag[key] \n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "def check_if_meta_tif(path):\n",
    "    \n",
    "    meta_dict = get_tif_meta(path)\n",
    "\n",
    "    # 'ImageDescription' key contains info about the file(s) in xml format   \n",
    "    tag_soup = str(meta_dict['ImageDescription'][0][21:])\n",
    "    root_meta = fromstring(tag_soup) # process xml string\n",
    "\n",
    "    # the 'image' tag in the xml is unique to the first tif with metadata; check for that\n",
    "    subdict_image = []\n",
    "    for neighbor in root_meta.iter('image'):\n",
    "         subdict_image = neighbor.attrib\n",
    "\n",
    "    return 'id' in subdict_image\n",
    "\n",
    "\n",
    "def threshold_img(data_thresh, thresh_percent=2):\n",
    "    \n",
    "    # get ranges for values across whole dataset and set a threshold to turn low amplitude noise to 0\n",
    "    data_range = [np.min(data_thresh), np.max(data_thresh)]\n",
    "    threshold_set_0 = data_range[1]*(thresh_percent/100.0) # set all values less than 1% of max signal to 0 with this threshold\n",
    "\n",
    "\n",
    "def assert_bruker(fpath):\n",
    "    meta_dict = get_tif_meta(fpath)\n",
    "    assert ('Prairie' in meta_dict['Software'][0]), \"This is not a bruker file!\"\n",
    "    \n",
    "    \n",
    "def load_save_composite_frames(save_object, glob_list, chunked_frame_idx, save_format):\n",
    "    # go through each chunk, load frames in chunk, process, and append to file\n",
    "    for idx, chunk_frames in enumerate(chunked_frame_idx):\n",
    "        print( 'Processing chunk {} out of {} chunks'.format(str(idx+1), str(len(chunked_frame_idx))) )\n",
    "        start_idx = chunk_frames[0]\n",
    "        end_idx = chunk_frames[-1]+1\n",
    "\n",
    "        #loaded_tiffs = uint16_scale(tiff.imread(glob_list[start_idx:end_idx], key=0, is_ome=True))\n",
    "        data_to_save = tiff.imread(glob_list[start_idx:end_idx], key=0, is_ome=True)\n",
    "\n",
    "        if save_format == 'tif':\n",
    "\n",
    "            for frame in tiffs_to_save:\n",
    "                save_object.save(frame, photometric='minisblack')\n",
    "\n",
    "        # https://stackoverflow.com/questions/25655588/incremental-writes-to-hdf5-with-h5py\n",
    "        elif save_format == 'h5':   \n",
    "\n",
    "            # append data to h5    \n",
    "            save_object[start_idx:end_idx] = data_to_save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_ometif_to_composite(fdir, fname, save_type='h5', num_frames=None):\n",
    "\n",
    "    save_fname = os.path.join(fdir, fname)\n",
    "    glob_list = glob.glob(os.path.join(fdir,\"*.tif\"))\n",
    "\n",
    "    # get frame info\n",
    "    if not num_frames: # CZ tmp: comment back in once make this into a function\n",
    "        num_frames = len(glob_list)\n",
    "    frame_shape = get_ometif_xy_shape(glob_list[0])\n",
    "    print(str(num_frames) + ' total frame')\n",
    "\n",
    "    # prepare to split data into chunks when loading to reduce memory imprint\n",
    "    chunk_size = 10000.0\n",
    "    n_chunks = int(np.ceil(num_frames/chunk_size))\n",
    "    chunked_frame_idx = np.array_split(np.arange(num_frames), n_chunks) # split frame indices into chunks\n",
    "\n",
    "    assert_bruker(glob_list[0])\n",
    "    print('Processing Bruker data')\n",
    "\n",
    "    # prepare handles to write data to\n",
    "    if save_type == 'tif':\n",
    "        save_object = tiff.TiffWriter(save_fname + '.tif', bigtiff=True)\n",
    "    elif save_type == 'h5':\n",
    "        f = h5py.File(save_fname + '.h5', 'w')\n",
    "        # get data shape and chunk up data, and initialize h5 \n",
    "        save_object = f.create_dataset('imaging', (num_frames, frame_shape[0], frame_shape[1]), \n",
    "                                maxshape=(None, frame_shape[0], frame_shape[1]), dtype='uint16')\n",
    "        \n",
    "    load_save_composite_frames(save_object, glob_list, chunked_frame_idx, save_type)\n",
    "    \n",
    "    if save_type == 'h5':\n",
    "        f.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "User-defined variables\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams = [\n",
    "            {\n",
    "                'fname': 'vj_ofc_imageactivate_001_2020903-001',   # \n",
    "                'fdir': r'D:\\bruker_data\\vj_ofc_imageactivate_001_20200903\\vj_ofc_imageactivate_001_2020903-001', #  \n",
    "                'save_type': 'h5',\n",
    "                'number_frames': None # optional; number of frames to analyze; defaults to analyzing whole session (None)\n",
    "            }\n",
    "        ]\n",
    "    elif method == 'f2a': # if string is empty, load predefined list of files in files_to_analyze_event\n",
    "\n",
    "        fparams = files_to_analyze_prepreprocess.define_fparams()\n",
    "\n",
    "    elif method == 'root_dir':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    return fparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_process(fparams):\n",
    "    \n",
    "    main_ometif_to_composite(fparams['fdir'], fparams['fname'], fparams['save_type'], num_frames=fparams['number_frames'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir'\n",
    "\n",
    "# determine number of cores to use and initialize parallel pool\n",
    "num_processes = min(mp.cpu_count(), num_files)\n",
    "print('Total CPU cores for parallel processing: ' + str(num_processes))\n",
    "pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "# perform parallel processing; pass iterable list of file params to the analysis module selection code\n",
    "#pool.map(single_file_process, fparams)\n",
    "\n",
    "## for testing\n",
    "for fparam in fparams:\n",
    "    single_file_process(fparam) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta & Behavioral Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables and load 2p recording xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import utils_bruker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'vj_ofc_imageactivate_001_2020903-001'\n",
    "fdir = r'D:\\bruker_data\\vj_ofc_imageactivate_001_20200903\\vj_ofc_imageactivate_001_2020903-001'\n",
    "\n",
    "analog_names = ['stim', 'frames', 'licks', 'rewards']\n",
    "\n",
    "flag_bruker_analog = True # set to true if analog/voltage input signals are present and are of interest\n",
    "flag_bruker_stim = True\n",
    "\n",
    "flag_multicondition_analog = False # if a single analog port contains multiple conditions that need to be split up, set to true \n",
    "behav_id_of_interest = [101,102,103]\n",
    "ai_to_split = 2 # int, analog port number that contains TTLs of multiple conditions; events here will be split into individual conditions if flag_multicondition_analog is set to true\n",
    "\n",
    "validation_plots = False # set to true if want to plot traces of ttl pulses for visualizing and validating\n",
    "valid_plot_channel = 'input_2' # analog dataframe column names get cleaned up; AI's are \"input_#\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths and output file names\n",
    "bruker_tseries_xml_path = os.path.join(fdir, fname + '.xml') # recording/tseries main xml\n",
    "\n",
    "glob_analog_csv = glob.glob(os.path.join(fdir,\"*_VoltageRecording_*.csv\")) # grab all analog/voltage recording csvs\n",
    "glob_analog_xml = glob.glob(os.path.join(fdir,\"*_VoltageRecording_*.xml\")) # grab all analog/voltage recording xml meta\n",
    "\n",
    "# behavioral event identification files\n",
    "behav_fname = fname + '_taste_reactivity.csv' # csv containing each behav event and corresponding sample\n",
    "behav_event_key_path = r'D:\\bruker_data\\Adam\\key_event.xlsx' # location of excel matching event names and id's\n",
    "\n",
    "behav_save_path = os.path.join(fdir, 'framenumberforevents_{}.pkl'.format(fname) )\n",
    "behav_analog_save_path = os.path.join(fdir, 'framenumberforevents_analog_{}.pkl'.format(fname) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in recording/tseries main xml and grab frame period\n",
    "def bruker_xml_get_2p_fs(xml_path):\n",
    "    xml_parse = ET.parse(xml_path).getroot()\n",
    "    for child in list(xml_parse.findall('PVStateShard')[0]):\n",
    "        if 'framePeriod' in ET.tostring(child):\n",
    "            return 1.0/float(child.attrib['value'])\n",
    "\n",
    "        \n",
    "# takes bruker xml data, parses for each frame's timing and cycle\n",
    "def bruker_xml_make_frame_info_df(xml_path):\n",
    "    xml_parse = ET.parse(xml_path).getroot()\n",
    "    frame_info_df = pd.DataFrame()\n",
    "    for idx, type_tag in enumerate(xml_parse.findall('Sequence/Frame')):\n",
    "        # extract relative and absolute time from each frame's xml meta data\n",
    "        frame_info_df.loc[idx, 'rel_time'] = float(type_tag.attrib['relativeTime'])\n",
    "        frame_info_df.loc[idx, 'abs_time'] = float(type_tag.attrib['absoluteTime'])\n",
    "\n",
    "        # grab cycle number from frame's name\n",
    "        frame_fname = type_tag.findall('File')[0].attrib['filename']\n",
    "        frame_info_df.loc[idx, 'cycle_num'] = int(re.findall('Cycle(\\d+)', frame_fname)[0])\n",
    "    return frame_info_df\n",
    "\n",
    "\n",
    "# loads and parses the analog/voltage recording's xml and grabs sampling rate\n",
    "def bruker_analog_xml_get_fs(xml_fpath):\n",
    "    analog_xml = ET.parse(xml_fpath).getroot()\n",
    "    return float(analog_xml.findall('Experiment')[0].find('Rate').text)\n",
    "\n",
    "\n",
    "# concatenate the analog input csv files if there are multiple cycles\n",
    "def bruker_concatenate_analog(fname, fpath):\n",
    "    # grab all csv voltage recording csv files that aren't the concatenated full\n",
    "    glob_analog_csv = [f for f in glob.glob(os.path.join(fpath,\"*_VoltageRecording_*.csv\")) if 'full' not in f]\n",
    "    glob_analog_xml = glob.glob(os.path.join(fpath,\"*_VoltageRecording_*.xml\"))\n",
    "\n",
    "    # xml's contain metadata about the analog csv; make sure sampling rate is consistent across cycles\n",
    "    analog_xml_fs = set(map(bruker_analog_xml_get_fs, glob_analog_xml)) # map grabs sampling rate across all cycle xmls; set finds all unique list entries  \n",
    "    if len(analog_xml_fs) > 1: \n",
    "          warnings.warn('Sampling rate is not consistent across cycles!')\n",
    "    else:\n",
    "        analog_fs = list(analog_xml_fs)[0]\n",
    "    \n",
    "    # cycle through analog csvs and append to a dataframe\n",
    "    analog_concat = pd.DataFrame()\n",
    "    for cycle_idx, cycle_path_csv in enumerate(glob_analog_csv):\n",
    "\n",
    "        cycle_df = pd.read_csv(cycle_path_csv)\n",
    "        num_samples = len(cycle_df['Time(ms)'])\n",
    "        cycle_df['Time(s)'] = cycle_df['Time(ms)']/1000.0\n",
    "\n",
    "        cycle_df['cycle_num'] = float(re.findall('Cycle(\\d+)', cycle_path_csv)[0]) # get cycle # from filename\n",
    "        if cycle_idx == 0: # initialize pd dataframe with first cycle's data\n",
    "            cycle_df['cumulative_time_ms'] = cycle_df['Time(ms)'].values\n",
    "            analog_concat = cycle_df\n",
    "        else:\n",
    "            # since time resets for each cycle (if more than one), calculate cumulative time\n",
    "            last_cumulative_time = analog_concat['cumulative_time_ms'].iloc[-1]\n",
    "            cycle_df['cumulative_time_ms'] = cycle_df['Time(ms)'].values + last_cumulative_time + 1 # add 1 so that new cycle's first sample isn't the same as the last cycle's last sample\n",
    "            analog_concat = analog_concat.append(cycle_df, ignore_index = True)\n",
    "    \n",
    "    # clean up column names\n",
    "    analog_concat.columns = analog_concat.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '_').str.replace(')', '')\n",
    "        \n",
    "    # loop through all analog columns and get the diff and threshold for event onsets\n",
    "    analog_column_names = [column for column in analog_concat.columns if 'input' in column]\n",
    "    num_analogs = len(analog_column_names)   \n",
    "    for analog_column_name in analog_column_names:\n",
    "        analog_concat[analog_column_name + '_diff'] = np.append(np.diff(analog_concat[analog_column_name]) > 0.07, \n",
    "                                                                False) # add a false to match existing df length\n",
    "\n",
    "    # save concatenated analog csv        \n",
    "    save_full_csv_path = os.path.join(fpath, fname + '_VoltageRecording_full.csv')\n",
    "    analog_concat.to_csv(save_full_csv_path, index=False)\n",
    "\n",
    "    return analog_concat\n",
    "\n",
    "\n",
    "# function for finding the index of the closest entry in an array to a provided value\n",
    "def find_nearest_idx(array, value):\n",
    "\n",
    "    if isinstance(array, pd.Series):\n",
    "        idx = (np.abs(array - value)).idxmin()\n",
    "        return idx, array.index.get_loc(idx), array[idx] # series index, 0-relative index, entry value\n",
    "    else:\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx, array[idx]\n",
    "\n",
    "\n",
    "### Take in analog dataframe (contains analog tseries and thresholded boolean) and make dict of 2p frame times for each condition's event\n",
    "def match_analog_event_to_2p(imaging_info_df, analog_dataframe, rename_ports = None, flag_multicondition_analog = False): \n",
    "\n",
    "    analog_event_dict = {} # will contain analog channel names as keys and 2p imaging frame numbers for each event/ttl onset\n",
    "    analog_event_samples = {}\n",
    "    all_diff_columns = [diff_column for diff_column in analog_df.columns if 'diff' in diff_column] # grab all diff'd analog column names\n",
    "\n",
    "    for ai_diff in sorted(all_diff_columns):\n",
    "        \n",
    "        # if user gives ports to rename, grab port data name\n",
    "        if rename_ports:\n",
    "            ai_port_num = int(re.findall('\\d+', ai_diff )[0])\n",
    "            ai_name = rename_ports[ai_port_num]\n",
    "        else:\n",
    "            ai_name = ai_diff\n",
    "        \n",
    "        if flag_multicondition_analog: # if the trials in analog ports need to be split up later, make a subdict to accommodate conditions keys\n",
    "            analog_event_dict[ai_name] = {}; analog_event_dict[ai_name]['all'] = []\n",
    "            analog_event_samples[ai_name] = {}; analog_event_samples[ai_name]['all'] = []\n",
    "        else:\n",
    "            analog_event_dict[ai_name] = []\n",
    "            analog_event_samples[ai_name] = [] \n",
    "            \n",
    "        # grab analog samples where TTL onset occurred\n",
    "        # analog_df diff columns are booleans for each frame that indicate if TTL threshold crossed (ie. event occurred)\n",
    "        analog_events = analog_df.loc[analog_df[ai_diff] == True, ['time_s', 'cycle_num']] \n",
    "\n",
    "        # for each detected analog event, find nearest 2p frame index and add to analog event dict\n",
    "        \n",
    "        for idx, analog_event in analog_events.iterrows():\n",
    "\n",
    "            this_cycle_imaging_info = imaging_info_df[imaging_info_df['cycle_num'] == analog_event['cycle_num']]\n",
    "            \n",
    "            whole_session_idx, cycle_relative_idx, value = find_nearest_idx(this_cycle_imaging_info['rel_time'], analog_event['time_s'])\n",
    "\n",
    "            if flag_multicondition_analog:\n",
    "                analog_event_dict[ai_name]['all'].append(whole_session_idx)\n",
    "                analog_event_samples[ai_name]['all'].append(idx)\n",
    "            else:\n",
    "                analog_event_dict[ai_name].append(whole_session_idx)\n",
    "                analog_event_samples[ai_name].append(idx)\n",
    "\n",
    "    return analog_event_dict, analog_event_samples\n",
    "\n",
    "    \n",
    "# if all behav events of interest (different conditions) are recorded on a single AI channel\n",
    "# and need to reference the behavioral events csv to split conditions up\n",
    "def split_analog_channel(ai_to_split, fdir, behav_fname, behav_event_key_path, analog_event_dict):\n",
    "\n",
    "    unicode_to_str = lambda x:str(x) # just a simple function to convert unicode to string; \n",
    "\n",
    "    this_ai_to_split = [analog_diff_name for analog_diff_name in analog_event_dict.keys() if str(ai_to_split) in analog_diff_name][0]\n",
    "    \n",
    "    # load id's and samples (camera samples?) of behavioral events (output by behavioral program)\n",
    "    behav_df = pd.read_csv(os.path.join(fdir, behav_fname), names=['id', 'sample'])\n",
    "    behav_event_keys = pd.read_excel(behav_event_key_path)\n",
    "\n",
    "    # using the behav event id, grab the event name from the keys dataframe; names are in unicode, so have to convert to string\n",
    "    behav_name_of_interest = map(unicode_to_str, \n",
    "                                 behav_event_keys[behav_event_keys['event_id'].isin(behav_id_of_interest)]['event_desc'].values)\n",
    "\n",
    "    # go into ordered behavioral event df, grab the trials with condition IDs of 'behav_id_of_interest' in order\n",
    "    trial_ids = list(behav_df[behav_df['id'].isin(behav_id_of_interest)]['id'].values) # grab 101, 102, 103 trials in order\n",
    "    \n",
    "    # loop through behav conditions, and separate event times for the conglomerate event times in analog_event_dict\n",
    "    for behav_event_id, behav_event_name in zip(behav_id_of_interest, behav_name_of_interest):\n",
    "        this_event_idxs = [idx for idx,val in enumerate(trial_ids) if val==behav_event_id]\n",
    "        analog_event_dict[this_ai_to_split][behav_event_name] = [analog_event_dict[this_ai_to_split]['all'][idx] for idx in this_event_idxs]\n",
    "        # analog_event_dict ultimately contains 2p frame indices for each event categorized by condition\n",
    "\n",
    "    # save preprocessed behavioral event data\n",
    "    with open(behav_analog_save_path, 'wb') as handle:\n",
    "        pickle.dump(analog_event_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# take in data from an analog input and plot detected ttls\n",
    "def plot_analog_validation(AI_onsets, analog_tseries, analog_fs, save_dir = None):\n",
    "    # following is just for visualizing ttls; here make tiles for indexing and extracting ttl data in trial manner\n",
    "    num_AI = len(AI_onsets)\n",
    "    rel_ind_vec = np.arange(-0.5*analog_fs, 3*analog_fs, 1)\n",
    "    rel_ind_tile = np.tile(rel_ind_vec, (num_AI,1))\n",
    "    AI_onset_tile = np.tile(AI_onsets, (len(rel_ind_vec),1)).T\n",
    "\n",
    "    # extract analog values across flattened trial indices, get values of series, then reshape to 2d array\n",
    "    AI_value_tile = analog_tseries[np.ndarray.flatten(AI_onset_tile + rel_ind_tile)].values.reshape(AI_onset_tile.shape)\n",
    "    if AI_value_tile.shape[0] == num_AI:\n",
    "        AI_value_tile = AI_value_tile.T\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3,figsize=(17,5))\n",
    "\n",
    "    ax[0].set_title('Full TTL series')\n",
    "    ax[0].plot(analog_tseries)\n",
    "\n",
    "    ax[1].set_title('{} ttls detected'.format(num_AI))\n",
    "    ax[1].plot( AI_value_tile );\n",
    "    ax[1].set_xlabel('Time (ms)')\n",
    "    ax[1].set_ylabel('Volts');\n",
    "\n",
    "    svec = np.arange(0, 15*analog_fs)\n",
    "    tvec_plot = svec/analog_fs\n",
    "    ax[2].set_title('Specific window (first 15s)')\n",
    "    ax[2].plot(tvec_plot, analog_tseries[svec])\n",
    "    ax[2].set_xlabel('Seconds')\n",
    "    \n",
    "    if save_path:\n",
    "        utils.check_exist_dir(save_dir)\n",
    "        fig.savefig(os.path.join(save_dir, 'ttl_validation.png'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Parse main time-series xml, 2) extract frame timing and cycle info into a pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def make_imaging_info_df(bruker_tseries_xml_path):\n",
    "xml_parse = ET.parse(bruker_tseries_xml_path).getroot()\n",
    "frame_info_df = pd.DataFrame()\n",
    "type_tags = xml_parse.findall('Sequence/Frame')\n",
    "\n",
    "# lambda function to take in a list of xml frame meta data and pull out timing and cycle info \n",
    "grab_2p_xml_frame_time = lambda type_tag: [float(type_tag.attrib['relativeTime']), \n",
    "                                           float(type_tag.attrib['absoluteTime']),\n",
    "                                           int(re.findall('Cycle(\\d+)', type_tag.findall('File')[0].attrib['filename'])[0]) # first grab this frame's file name, then use regex to grab cycle number in the fname\n",
    "                                          ] \n",
    "\n",
    "# make a dataframe of relative time, absolute time, cycle number for each frame\n",
    "imaging_info_df = pd.DataFrame(map(grab_2p_xml_frame_time, type_tags), columns=['rel_time', 'abs_time', 'cycle_num'])\n",
    "\n",
    "# return imaging_info_df\n",
    "\n",
    "# get more timing meta data\n",
    "fs_2p = bruker_xml_get_2p_fs(bruker_tseries_xml_path)\n",
    "tvec_2p = imaging_info_df['rel_time']\n",
    "num_frames_2p = len(tvec_2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process analog voltage recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have analog signals, that indicate behavioral event onset, sent from your behavioral DAQ to the bruker GPIO box, the following code:\n",
    "\n",
    "1) parses the analog voltage recording xmls \n",
    "2) extracts the signals from the csvs\n",
    "3) extracts the TTL onset times\n",
    "4) and finally lines up which frame the TTL occurred on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frames': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  14,\n",
       "  16,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  22,\n",
       "  23,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  29,\n",
       "  31,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  46,\n",
       "  48,\n",
       "  50,\n",
       "  52,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  58,\n",
       "  59,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  65,\n",
       "  67,\n",
       "  69,\n",
       "  71,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  80,\n",
       "  82,\n",
       "  84,\n",
       "  86,\n",
       "  88,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  95,\n",
       "  97,\n",
       "  99,\n",
       "  101,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  114,\n",
       "  116,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  122,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  129,\n",
       "  131,\n",
       "  133,\n",
       "  135,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  144,\n",
       "  146,\n",
       "  148,\n",
       "  150,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  163,\n",
       "  165,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  171,\n",
       "  172,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  178,\n",
       "  180,\n",
       "  182,\n",
       "  184,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  191,\n",
       "  193,\n",
       "  195,\n",
       "  197,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  206,\n",
       "  208,\n",
       "  210,\n",
       "  212,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  225,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  238,\n",
       "  240,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  253,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  266,\n",
       "  268,\n",
       "  270,\n",
       "  272,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  279,\n",
       "  281,\n",
       "  283,\n",
       "  285,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  294,\n",
       "  296,\n",
       "  298,\n",
       "  300,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  307,\n",
       "  309,\n",
       "  311,\n",
       "  313,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  320,\n",
       "  322,\n",
       "  324,\n",
       "  326,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  332,\n",
       "  333,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  339,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  345,\n",
       "  346,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  352,\n",
       "  354,\n",
       "  356,\n",
       "  358,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  365,\n",
       "  367,\n",
       "  369,\n",
       "  371,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  378,\n",
       "  380,\n",
       "  382,\n",
       "  384,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  391,\n",
       "  393,\n",
       "  395,\n",
       "  397,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  406,\n",
       "  408,\n",
       "  410,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  419,\n",
       "  421,\n",
       "  423,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  432,\n",
       "  434,\n",
       "  436,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  449,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  462,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  468,\n",
       "  469,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  475,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  481,\n",
       "  482,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  488,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  494,\n",
       "  495,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  501,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  507,\n",
       "  508,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  514,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  527,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  538,\n",
       "  540,\n",
       "  542,\n",
       "  544,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  551,\n",
       "  553,\n",
       "  555,\n",
       "  557,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  564,\n",
       "  566,\n",
       "  568,\n",
       "  570,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  577,\n",
       "  579,\n",
       "  581,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  590,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  603,\n",
       "  605,\n",
       "  607,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  614,\n",
       "  616,\n",
       "  618,\n",
       "  620,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  627,\n",
       "  629,\n",
       "  631,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  640,\n",
       "  642,\n",
       "  644,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  653,\n",
       "  655,\n",
       "  657,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  664,\n",
       "  666,\n",
       "  668,\n",
       "  670,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  677,\n",
       "  679,\n",
       "  681,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  690,\n",
       "  692,\n",
       "  694,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  701,\n",
       "  703,\n",
       "  705,\n",
       "  707,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  714,\n",
       "  716,\n",
       "  718,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  727,\n",
       "  729,\n",
       "  731,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  738,\n",
       "  740,\n",
       "  742,\n",
       "  744,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  751,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  762,\n",
       "  764,\n",
       "  766,\n",
       "  768,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  775,\n",
       "  777,\n",
       "  779,\n",
       "  781,\n",
       "  782,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  788,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  799,\n",
       "  801,\n",
       "  803,\n",
       "  805,\n",
       "  806,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  812,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  818,\n",
       "  819,\n",
       "  821,\n",
       "  823,\n",
       "  825,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  836,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  842,\n",
       "  843,\n",
       "  845,\n",
       "  847,\n",
       "  849,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  860,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  869,\n",
       "  871,\n",
       "  873,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  880,\n",
       "  882,\n",
       "  884,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  893,\n",
       "  895,\n",
       "  897,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  904,\n",
       "  906,\n",
       "  908,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  915,\n",
       "  917,\n",
       "  919,\n",
       "  921,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  928,\n",
       "  930,\n",
       "  932,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  939,\n",
       "  941,\n",
       "  943,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  952,\n",
       "  954,\n",
       "  956,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  963,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  974,\n",
       "  976,\n",
       "  978,\n",
       "  980,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  987,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  998,\n",
       "  1000,\n",
       "  1002,\n",
       "  1004,\n",
       "  1005,\n",
       "  1007,\n",
       "  1008,\n",
       "  1009,\n",
       "  1011,\n",
       "  1013,\n",
       "  1014,\n",
       "  1015,\n",
       "  1018,\n",
       "  1020,\n",
       "  1022,\n",
       "  1024,\n",
       "  1025,\n",
       "  1026,\n",
       "  1027,\n",
       "  1028,\n",
       "  1029,\n",
       "  1031,\n",
       "  1033,\n",
       "  1035,\n",
       "  1037,\n",
       "  1038,\n",
       "  1039,\n",
       "  1040,\n",
       "  1042,\n",
       "  1044,\n",
       "  1046,\n",
       "  1048,\n",
       "  1049,\n",
       "  1050,\n",
       "  1051,\n",
       "  1053,\n",
       "  1055,\n",
       "  1057,\n",
       "  1059,\n",
       "  1060,\n",
       "  1061,\n",
       "  1062,\n",
       "  1063,\n",
       "  1064,\n",
       "  1066,\n",
       "  1068,\n",
       "  1070,\n",
       "  1072,\n",
       "  1073,\n",
       "  1074,\n",
       "  1075,\n",
       "  1077,\n",
       "  1079,\n",
       "  1081,\n",
       "  1083,\n",
       "  1084,\n",
       "  1085,\n",
       "  1086,\n",
       "  1088,\n",
       "  1090,\n",
       "  1092,\n",
       "  1094,\n",
       "  1095,\n",
       "  1096,\n",
       "  1097,\n",
       "  1098,\n",
       "  1099,\n",
       "  1101,\n",
       "  1103,\n",
       "  1105,\n",
       "  1108,\n",
       "  1109,\n",
       "  1110,\n",
       "  1112,\n",
       "  1114,\n",
       "  1115,\n",
       "  1116,\n",
       "  1119,\n",
       "  1120,\n",
       "  1121,\n",
       "  1123,\n",
       "  1125,\n",
       "  1126,\n",
       "  1127,\n",
       "  1129,\n",
       "  1130,\n",
       "  1132,\n",
       "  1134,\n",
       "  1136,\n",
       "  1138,\n",
       "  1139,\n",
       "  1140,\n",
       "  1141,\n",
       "  1143,\n",
       "  1144,\n",
       "  1145,\n",
       "  1147,\n",
       "  1149,\n",
       "  1150,\n",
       "  1151,\n",
       "  1154,\n",
       "  1156,\n",
       "  1158,\n",
       "  1160,\n",
       "  1161,\n",
       "  1162,\n",
       "  1163,\n",
       "  1164,\n",
       "  1165,\n",
       "  1167,\n",
       "  1169,\n",
       "  1171,\n",
       "  1173,\n",
       "  1174,\n",
       "  1175,\n",
       "  1176,\n",
       "  1178,\n",
       "  1180,\n",
       "  1182,\n",
       "  1184,\n",
       "  1185,\n",
       "  1186,\n",
       "  1187,\n",
       "  1189,\n",
       "  1191,\n",
       "  1193,\n",
       "  1195,\n",
       "  1196,\n",
       "  1197,\n",
       "  1198,\n",
       "  1200,\n",
       "  1202,\n",
       "  1204,\n",
       "  1206,\n",
       "  1207,\n",
       "  1208,\n",
       "  1209,\n",
       "  1211,\n",
       "  1213,\n",
       "  1215,\n",
       "  1217,\n",
       "  1218,\n",
       "  1220,\n",
       "  1221,\n",
       "  1222,\n",
       "  1224,\n",
       "  1226,\n",
       "  1227,\n",
       "  1228,\n",
       "  1231,\n",
       "  1232,\n",
       "  1233,\n",
       "  1235,\n",
       "  1237,\n",
       "  1238,\n",
       "  1239,\n",
       "  1242,\n",
       "  1244,\n",
       "  1246,\n",
       "  1248,\n",
       "  1249,\n",
       "  1250,\n",
       "  1251,\n",
       "  1252,\n",
       "  1253,\n",
       "  1255,\n",
       "  1257,\n",
       "  1259,\n",
       "  1261,\n",
       "  1262,\n",
       "  1263,\n",
       "  1264,\n",
       "  1266,\n",
       "  1268,\n",
       "  1270,\n",
       "  1272,\n",
       "  1273,\n",
       "  1274,\n",
       "  1275,\n",
       "  1277,\n",
       "  1279,\n",
       "  1281,\n",
       "  1283,\n",
       "  1284,\n",
       "  1285,\n",
       "  1286,\n",
       "  1288,\n",
       "  1290,\n",
       "  1292,\n",
       "  1294,\n",
       "  1295,\n",
       "  1296,\n",
       "  1297,\n",
       "  1299,\n",
       "  1301,\n",
       "  1303,\n",
       "  1305,\n",
       "  1306,\n",
       "  1307,\n",
       "  1308,\n",
       "  1309,\n",
       "  1310,\n",
       "  1312,\n",
       "  1314,\n",
       "  1316,\n",
       "  1319,\n",
       "  1320,\n",
       "  1321,\n",
       "  1323,\n",
       "  1325,\n",
       "  1326,\n",
       "  1327,\n",
       "  1330,\n",
       "  1331,\n",
       "  1332,\n",
       "  1334,\n",
       "  1336,\n",
       "  1337,\n",
       "  1338,\n",
       "  1341,\n",
       "  1343,\n",
       "  1345,\n",
       "  1347,\n",
       "  1348,\n",
       "  1349,\n",
       "  1350,\n",
       "  1351,\n",
       "  1352,\n",
       "  1354,\n",
       "  1356,\n",
       "  1358,\n",
       "  1359,\n",
       "  1360,\n",
       "  1361,\n",
       "  1362,\n",
       "  1363,\n",
       "  1365,\n",
       "  1367,\n",
       "  1369,\n",
       "  1371,\n",
       "  1372,\n",
       "  1373,\n",
       "  1374,\n",
       "  1376,\n",
       "  1378,\n",
       "  1380,\n",
       "  1383,\n",
       "  1384,\n",
       "  1385,\n",
       "  1387,\n",
       "  1389,\n",
       "  1390,\n",
       "  1391,\n",
       "  1394,\n",
       "  1395,\n",
       "  1396,\n",
       "  1398,\n",
       "  1400,\n",
       "  1401,\n",
       "  1402,\n",
       "  1404,\n",
       "  1405,\n",
       "  1406,\n",
       "  1407,\n",
       "  1409,\n",
       "  1411,\n",
       "  1412,\n",
       "  1413,\n",
       "  1415,\n",
       "  1416,\n",
       "  1418,\n",
       "  1420,\n",
       "  1422,\n",
       "  1424,\n",
       "  1425,\n",
       "  1426,\n",
       "  1427,\n",
       "  1429,\n",
       "  1431,\n",
       "  1433,\n",
       "  1435,\n",
       "  1436,\n",
       "  1437,\n",
       "  1438,\n",
       "  1440,\n",
       "  1442,\n",
       "  1444,\n",
       "  1446,\n",
       "  1447,\n",
       "  1448,\n",
       "  1449,\n",
       "  1451,\n",
       "  1453,\n",
       "  1455,\n",
       "  1458,\n",
       "  1459,\n",
       "  1460,\n",
       "  1462,\n",
       "  1464,\n",
       "  1465,\n",
       "  1466,\n",
       "  1468,\n",
       "  1469,\n",
       "  1470,\n",
       "  1471,\n",
       "  1473,\n",
       "  1475,\n",
       "  1476,\n",
       "  1477,\n",
       "  1479,\n",
       "  1480,\n",
       "  1481,\n",
       "  1482,\n",
       "  1484,\n",
       "  1486,\n",
       "  1488,\n",
       "  1490,\n",
       "  1491,\n",
       "  1493,\n",
       "  1495,\n",
       "  1497,\n",
       "  1499,\n",
       "  1500,\n",
       "  1501,\n",
       "  1502,\n",
       "  1504,\n",
       "  1506,\n",
       "  1508,\n",
       "  1510,\n",
       "  1511,\n",
       "  1512,\n",
       "  1513,\n",
       "  1515,\n",
       "  1517,\n",
       "  1519,\n",
       "  1521,\n",
       "  1522,\n",
       "  1523,\n",
       "  1524,\n",
       "  1526,\n",
       "  1528,\n",
       "  1530,\n",
       "  1533,\n",
       "  1534,\n",
       "  1535,\n",
       "  1537,\n",
       "  1539,\n",
       "  1541,\n",
       "  1544,\n",
       "  1545,\n",
       "  1546,\n",
       "  1548,\n",
       "  1550,\n",
       "  1551,\n",
       "  1552,\n",
       "  1554,\n",
       "  1555,\n",
       "  1556,\n",
       "  1557,\n",
       "  ...],\n",
       " 'licks': [9,\n",
       "  85,\n",
       "  140,\n",
       "  280,\n",
       "  283,\n",
       "  285,\n",
       "  298,\n",
       "  314,\n",
       "  317,\n",
       "  426,\n",
       "  472,\n",
       "  629,\n",
       "  832,\n",
       "  834,\n",
       "  836,\n",
       "  838,\n",
       "  841,\n",
       "  861,\n",
       "  894,\n",
       "  917,\n",
       "  1020,\n",
       "  1070,\n",
       "  1106,\n",
       "  1200,\n",
       "  1268,\n",
       "  1327,\n",
       "  1329,\n",
       "  1332,\n",
       "  1334,\n",
       "  1336,\n",
       "  1348,\n",
       "  1381,\n",
       "  1438,\n",
       "  1451,\n",
       "  1477,\n",
       "  1514,\n",
       "  1556,\n",
       "  1732,\n",
       "  1734,\n",
       "  1737,\n",
       "  1739,\n",
       "  1751,\n",
       "  1835,\n",
       "  1875,\n",
       "  1881,\n",
       "  1953,\n",
       "  2034,\n",
       "  2066,\n",
       "  2205,\n",
       "  2291,\n",
       "  2309,\n",
       "  2311,\n",
       "  2314,\n",
       "  2316,\n",
       "  2406,\n",
       "  2437,\n",
       "  2504,\n",
       "  2692,\n",
       "  2772,\n",
       "  2783,\n",
       "  2789,\n",
       "  2845,\n",
       "  2845,\n",
       "  2850,\n",
       "  2852,\n",
       "  2855,\n",
       "  2862,\n",
       "  2868,\n",
       "  2907,\n",
       "  2968,\n",
       "  2982,\n",
       "  3040,\n",
       "  3041,\n",
       "  3051,\n",
       "  3138,\n",
       "  3191,\n",
       "  3205,\n",
       "  3325,\n",
       "  3418,\n",
       "  3421,\n",
       "  3423,\n",
       "  3425,\n",
       "  3427,\n",
       "  3429,\n",
       "  3430,\n",
       "  3457,\n",
       "  3470,\n",
       "  3535,\n",
       "  3580,\n",
       "  3637,\n",
       "  3651,\n",
       "  3748,\n",
       "  3753,\n",
       "  3754,\n",
       "  3754,\n",
       "  3756,\n",
       "  3759,\n",
       "  3761,\n",
       "  3763,\n",
       "  3766,\n",
       "  3776,\n",
       "  3778,\n",
       "  3883,\n",
       "  3886,\n",
       "  3889,\n",
       "  3893,\n",
       "  3913,\n",
       "  3936,\n",
       "  3976,\n",
       "  4071,\n",
       "  4073,\n",
       "  4075,\n",
       "  4077,\n",
       "  4078,\n",
       "  4080,\n",
       "  4083,\n",
       "  4173,\n",
       "  4251,\n",
       "  4269,\n",
       "  4374,\n",
       "  4431,\n",
       "  4504,\n",
       "  4515,\n",
       "  4547,\n",
       "  4593,\n",
       "  4595,\n",
       "  4597,\n",
       "  4600,\n",
       "  4602,\n",
       "  4605,\n",
       "  4777,\n",
       "  4823,\n",
       "  4885,\n",
       "  4937,\n",
       "  4939,\n",
       "  4941,\n",
       "  4943,\n",
       "  4955,\n",
       "  5379,\n",
       "  5384,\n",
       "  5386,\n",
       "  5387,\n",
       "  5390,\n",
       "  5392,\n",
       "  5395,\n",
       "  5498,\n",
       "  5518,\n",
       "  5520,\n",
       "  5520,\n",
       "  5522,\n",
       "  5525,\n",
       "  5527,\n",
       "  5530,\n",
       "  5534,\n",
       "  5537,\n",
       "  5540,\n",
       "  5635,\n",
       "  5716,\n",
       "  5736,\n",
       "  5805,\n",
       "  5807,\n",
       "  5887,\n",
       "  5889,\n",
       "  5891,\n",
       "  5891,\n",
       "  5894,\n",
       "  6036,\n",
       "  6038,\n",
       "  6041,\n",
       "  6083,\n",
       "  6112,\n",
       "  6146,\n",
       "  6176,\n",
       "  6244,\n",
       "  6265,\n",
       "  6299,\n",
       "  6382,\n",
       "  6429,\n",
       "  6432,\n",
       "  6435,\n",
       "  6437,\n",
       "  6440,\n",
       "  6442,\n",
       "  6445,\n",
       "  6483,\n",
       "  6488,\n",
       "  6559,\n",
       "  6564,\n",
       "  6568,\n",
       "  6604,\n",
       "  6676,\n",
       "  6744,\n",
       "  6779,\n",
       "  6823,\n",
       "  6826,\n",
       "  6852,\n",
       "  6854,\n",
       "  6856,\n",
       "  6859,\n",
       "  6862,\n",
       "  6864,\n",
       "  6912,\n",
       "  6961,\n",
       "  6981,\n",
       "  6984,\n",
       "  6985,\n",
       "  7021,\n",
       "  7060,\n",
       "  7097,\n",
       "  7160,\n",
       "  7164,\n",
       "  7200,\n",
       "  7202,\n",
       "  7204,\n",
       "  7207,\n",
       "  7380,\n",
       "  7381,\n",
       "  7531,\n",
       "  7596,\n",
       "  7627,\n",
       "  7731,\n",
       "  7735,\n",
       "  7737,\n",
       "  7739,\n",
       "  7741,\n",
       "  7744,\n",
       "  7746,\n",
       "  7759,\n",
       "  7762,\n",
       "  7765,\n",
       "  7849,\n",
       "  7851,\n",
       "  7854,\n",
       "  7867,\n",
       "  8254,\n",
       "  8256,\n",
       "  8259,\n",
       "  8261,\n",
       "  8322,\n",
       "  8480,\n",
       "  8588,\n",
       "  8719,\n",
       "  8723,\n",
       "  8725,\n",
       "  8727,\n",
       "  8730,\n",
       "  8733,\n",
       "  8735,\n",
       "  8737,\n",
       "  8739,\n",
       "  8755,\n",
       "  8791,\n",
       "  8793,\n",
       "  8833,\n",
       "  8847,\n",
       "  8851,\n",
       "  8884,\n",
       "  8887,\n",
       "  8939,\n",
       "  8973,\n",
       "  9039,\n",
       "  9162,\n",
       "  9227,\n",
       "  9229,\n",
       "  9231,\n",
       "  9234,\n",
       "  9294,\n",
       "  9338,\n",
       "  9387,\n",
       "  9391,\n",
       "  9504,\n",
       "  9543,\n",
       "  9607,\n",
       "  9708,\n",
       "  9726,\n",
       "  9731,\n",
       "  9733,\n",
       "  9735,\n",
       "  9738,\n",
       "  9740,\n",
       "  9842,\n",
       "  9845,\n",
       "  9987,\n",
       "  10097,\n",
       "  10099,\n",
       "  10102,\n",
       "  10104,\n",
       "  10106,\n",
       "  10195,\n",
       "  10321,\n",
       "  10449,\n",
       "  10493,\n",
       "  10516,\n",
       "  10520,\n",
       "  10522,\n",
       "  10524,\n",
       "  10526,\n",
       "  10529,\n",
       "  10531,\n",
       "  10719,\n",
       "  10818,\n",
       "  10874,\n",
       "  11060,\n",
       "  11120,\n",
       "  11123,\n",
       "  11125,\n",
       "  11128,\n",
       "  11171,\n",
       "  11175,\n",
       "  11229,\n",
       "  11317,\n",
       "  11353,\n",
       "  11662,\n",
       "  11680,\n",
       "  11683,\n",
       "  11686,\n",
       "  11688,\n",
       "  11737,\n",
       "  11792,\n",
       "  11982,\n",
       "  12005,\n",
       "  12007,\n",
       "  12007,\n",
       "  12009,\n",
       "  12012,\n",
       "  12095,\n",
       "  12133,\n",
       "  12136,\n",
       "  12209,\n",
       "  12257,\n",
       "  12318,\n",
       "  12349,\n",
       "  12395,\n",
       "  12424,\n",
       "  12428,\n",
       "  12430,\n",
       "  12432,\n",
       "  12435,\n",
       "  12437,\n",
       "  12556,\n",
       "  12556,\n",
       "  12573,\n",
       "  12640,\n",
       "  12730,\n",
       "  12784,\n",
       "  12902,\n",
       "  12991,\n",
       "  12994,\n",
       "  12995,\n",
       "  12998,\n",
       "  13000,\n",
       "  13002,\n",
       "  13005,\n",
       "  13074,\n",
       "  13150,\n",
       "  13165,\n",
       "  13238,\n",
       "  13376,\n",
       "  13380,\n",
       "  13382,\n",
       "  13384,\n",
       "  13386,\n",
       "  13389,\n",
       "  13391,\n",
       "  13465,\n",
       "  13507,\n",
       "  13600,\n",
       "  13692,\n",
       "  13711,\n",
       "  13714,\n",
       "  13716,\n",
       "  13718,\n",
       "  13721,\n",
       "  13867,\n",
       "  13869,\n",
       "  13938,\n",
       "  14112,\n",
       "  14117,\n",
       "  14119,\n",
       "  14121,\n",
       "  14124,\n",
       "  14126,\n",
       "  14157,\n",
       "  14388,\n",
       "  14443,\n",
       "  14537,\n",
       "  14658,\n",
       "  14689,\n",
       "  14693,\n",
       "  14695,\n",
       "  14698,\n",
       "  14700,\n",
       "  14702,\n",
       "  14705,\n",
       "  14707,\n",
       "  14735,\n",
       "  14817,\n",
       "  14831,\n",
       "  14831,\n",
       "  14953,\n",
       "  15047,\n",
       "  15078,\n",
       "  15103,\n",
       "  15105,\n",
       "  15107,\n",
       "  15109,\n",
       "  15228,\n",
       "  15265,\n",
       "  15324,\n",
       "  15386,\n",
       "  15559,\n",
       "  15563,\n",
       "  15565,\n",
       "  15568,\n",
       "  15570,\n",
       "  15572,\n",
       "  15575,\n",
       "  15577,\n",
       "  15614,\n",
       "  15617,\n",
       "  15908,\n",
       "  16058,\n",
       "  16069,\n",
       "  16072,\n",
       "  16074,\n",
       "  16076,\n",
       "  16079,\n",
       "  16109,\n",
       "  16224,\n",
       "  16229,\n",
       "  16277,\n",
       "  16295,\n",
       "  16360,\n",
       "  16438,\n",
       "  16466,\n",
       "  16470,\n",
       "  16472,\n",
       "  16474,\n",
       "  16477,\n",
       "  16479,\n",
       "  16490,\n",
       "  16502,\n",
       "  16521,\n",
       "  16523,\n",
       "  16599,\n",
       "  16601,\n",
       "  16620,\n",
       "  16991,\n",
       "  16993,\n",
       "  16995,\n",
       "  16998,\n",
       "  17129,\n",
       "  17349,\n",
       "  17448,\n",
       "  17474,\n",
       "  17478,\n",
       "  17480,\n",
       "  17482,\n",
       "  17482,\n",
       "  17484,\n",
       "  17487,\n",
       "  17490,\n",
       "  17493,\n",
       "  17624,\n",
       "  17628,\n",
       "  17727,\n",
       "  17730,\n",
       "  17733,\n",
       "  17782,\n",
       "  17851,\n",
       "  17855,\n",
       "  17876,\n",
       "  17880,\n",
       "  17940,\n",
       "  17943,\n",
       "  17945,\n",
       "  17948],\n",
       " 'rewards': [272,\n",
       "  826,\n",
       "  1319,\n",
       "  1724,\n",
       "  2301,\n",
       "  2845,\n",
       "  3412,\n",
       "  3738,\n",
       "  4065,\n",
       "  4579,\n",
       "  4928,\n",
       "  5372,\n",
       "  5881,\n",
       "  6424,\n",
       "  6844,\n",
       "  7194,\n",
       "  7727,\n",
       "  8247,\n",
       "  8714,\n",
       "  9221,\n",
       "  9720,\n",
       "  10093,\n",
       "  10094,\n",
       "  10513,\n",
       "  11112,\n",
       "  11571,\n",
       "  11999,\n",
       "  12000,\n",
       "  12420,\n",
       "  12988,\n",
       "  13371,\n",
       "  13692,\n",
       "  14099,\n",
       "  14680,\n",
       "  15095,\n",
       "  15556,\n",
       "  16061,\n",
       "  16462,\n",
       "  16984,\n",
       "  17471,\n",
       "  17933],\n",
       " 'stim': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analog_event_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run following if bruker analog signals are of interest\n",
    "if flag_bruker_analog:\n",
    "    \n",
    "    ### get analog data sampling rate from xml\n",
    "    analog_fs = bruker_analog_xml_get_fs(glob_analog_xml[0])\n",
    "\n",
    "    ### either load concatenated voltage recording (across cycles), perform the concatenation, or load a single CSV (for single cycle)\n",
    "    volt_rec_full_path = os.path.join(fdir, fname + '_VoltageRecording_full.csv')\n",
    "    if os.path.exists(volt_rec_full_path): # if a trial-stitched voltage recording was previously saved\n",
    "        analog_df = pd.read_csv(volt_rec_full_path)\n",
    "    else:\n",
    "        analog_df = bruker_concatenate_analog(fname, fdir) \n",
    "        \n",
    "    ### match analog ttl event onsets to the corresponding 2p frame (for each event in each analog port)\n",
    "    analog_event_dict, analog_event_samples = match_analog_event_to_2p(imaging_info_df, analog_df, rename_ports = analog_names)\n",
    "    \n",
    "    ### if there are multiple conditions signaled on a single analog port, split them up\n",
    "    if flag_multicondition_analog:\n",
    "        split_analog_channel(ai_to_split, fdir, behav_fname, behav_event_key_path, analog_event_dict)\n",
    "    \n",
    "    if validation_plots:\n",
    "        valid_save_dir = os.path.join(fdir, fname+'_output_images')\n",
    "        utils_bruker.check_exist_dir(valid_save_dir)\n",
    "        plot_analog_validation(analog_event_samples, analog_df[valid_plot_channel], \n",
    "                               analog_fs, valid_save_dir);\n",
    "    \n",
    "    # save preprocessed behavioral event data\n",
    "    with open(behav_save_path, 'wb') as handle:\n",
    "        pickle.dump(analog_event_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load behav data and ID event onset 2p frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have a separate event recorder that is synchronized to the 2p microscope (via frame onset TTL from the GPIO output), you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_fs = 1000.0 # sampling rate of behavioral csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load id's and samples (camera samples?) of behavioral events (output by behavioral program)\n",
    "behav_df = pd.read_csv(os.path.join(fdir, behav_fname), names=['id', 'sample'])\n",
    "behav_event_keys = pd.read_excel(behav_event_key_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# behav camera pulses are synced to 2p frames. To synchronize event times with the 2p frames, need to normalize to the first \n",
    "# camera frame.\n",
    "try:\n",
    "    camera_pulse_event_id = behav_event_keys['event_id'][behav_event_keys['event_desc'] == 'camera pulse'].values[0]\n",
    "    first_cam_pulse_sample = behav_df[behav_df['id'] == camera_pulse_event_id].iloc[0]['sample']\n",
    "except:\n",
    "    print('No camera pulse events!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame_events_dict = {}\n",
    "\n",
    "# loop through each type of event\n",
    "for idx, row in behav_event_keys.iterrows():\n",
    "    \n",
    "    # grab event id\n",
    "    this_id_name = str(row['event_desc'])\n",
    "    # grab rows of behav dataframe with this event's id\n",
    "    this_id_rows = behav_df['id'].isin([row['event_id']])\n",
    "\n",
    "    # convert to seconds\n",
    "    event_times_seconds = (behav_df[this_id_rows]['sample'].values-first_cam_pulse_sample)/behav_fs \n",
    "    # first_cam_pulse_sample subtracted to zero times relative to first camera frame\n",
    "    \n",
    "    # using zero'd event times in seconds, find closest 2p frame sample index\n",
    "    frame_events_dict[this_id_name] = map(partial(find_nearest_idx, tvec_2p), event_times_seconds)\n",
    "    \n",
    "# save preprocessed behavioral event data\n",
    "with open(behav_save_path, 'wb') as handle:\n",
    "    pickle.dump(frame_events_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this is if you performed opto stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_bruker_stim:\n",
    "    bruker_marked_pts_process.main_detect_save_stim_frames(fdir, fname, detection_threshold=1.5, flag_plot_mk_pts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROI calcium trace  with TTLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division # make py2 act like py3 where int division turns into float\n",
    "import matplotlib\n",
    "#important for text to be detected when importing saved figures into illustrator\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\stuberadmin\\Documents\\GitHub\\NAPE_imaging_analysis\\in_development\")\n",
    "import utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams['fname'] = 'vj_ofc_imageactivate_001_20200828-003' # \n",
    "        fparams['fdir'] = r'D:\\bruker_data\\vj_ofc_imageactivate_001_20200828\\vj_ofc_imageactivate_001_20200828-003' #  \n",
    "    \n",
    "        # set the sampling rate\n",
    "        fparams['fs'] = 15\n",
    "        #if os.path.join(fparams['fdir'], ):\n",
    "        #    fparams['fs'] = \n",
    "\n",
    "        # trial windowing \n",
    "        fparams['trial_start_end'] = [-2, 5]\n",
    "        fparams['baseline_end'] = -0.2\n",
    "        fparams['event_dur'] = 0.46#0.46 # duration of stim/event in seconds\n",
    "\n",
    "        # session info\n",
    "        fparams['opto_blank_frame'] = True\n",
    "        \n",
    "        # analysis and plotting arguments\n",
    "        fparams['flag_npil_corr'] = True # declare which data to load in\n",
    "        fparams['flag_zscore'] = True # whether or not to z-score data for plots\n",
    "        \n",
    "        # ROI sorting \n",
    "        fparams['flag_sort_rois'] = False\n",
    "        if fparams['flag_sort_rois']:\n",
    "            fparams['user_sort_method'] = 'max_value' # peak_time or max_value\n",
    "            fparams['roi_sort_cond'] = 'slm_stim' # for roi-resolved heatmaps, which condition to sort ROIs by\n",
    "            \n",
    "        # errorbar and saving figures\n",
    "        fparams['flag_roi_trial_avg_errbar'] = True # toggle to show error bar on roi- and trial-averaged traces\n",
    "        fparams['flag_trial_avg_errbar'] = True # toggle to show error bars on the trial-avg traces\n",
    "        fparams['flag_save_figs'] = True\n",
    "        fparams['interesting_rois'] = [] #[ 0, 1, 2, 23, 22, 11, 9, 5, 6, 7, 3, 4, 8, 12, 14, 15, 16, 17] # [35, 30, 20, 4] #\n",
    "    \n",
    "    elif method == 'f2a': # if string is empty, load predefined list of files in files_to_analyze_event\n",
    "\n",
    "        fparams = files_to_analyze_event.define_fparams()\n",
    "\n",
    "    elif method == 'root_dir':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    return fparams\n",
    "\n",
    "fparams = define_params(method = 'single') # options are 'single', 'f2a', 'root_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fparams['flag_npil_corr'] == True:\n",
    "    signals_fpath = os.path.join(fparams['fdir'], \"{}_neuropil_corrected_signals*\".format(fparams['fname']))\n",
    "    \n",
    "else:\n",
    "    signals_fpath = os.path.join(fparams['fdir'], \"*_extractedsignals*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time-series data\n",
    "glob_signal_files = glob.glob(signals_fpath)\n",
    "if len(glob_signal_files) == 1:\n",
    "    signals = np.squeeze(np.load(glob_signal_files[0]))\n",
    "else:\n",
    "    print('Warning: No or multiple signal files detected; using first detected file')\n",
    "\n",
    "total_samples = len(signals[0,:])\n",
    "num_rois = signals.shape[0]\n",
    "full_tvec = np.linspace(0, total_samples/fparams['fs'], total_samples)\n",
    "\n",
    "analog_tvec = analog_df['time_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "iROI = 1\n",
    "\n",
    "fig_combine, ax_combine = plt.subplots(1,1, figsize=(13,7))\n",
    "ax_combine.plot(tvec_2p, signals[iROI,:])\n",
    "\n",
    "ax_combine.plot(analog_tvec, analog_df['input_3'].values*400)\n",
    "ax_combine.plot(analog_tvec, analog_df['input_2'].values*30)\n",
    "ax_combine.plot(analog_tvec, analog_df['input_0'].values*1000)\n",
    "ax_combine.legend(['Activity', 'reward', 'licks', 'stim'], fontsize=15)\n",
    "\n",
    "ax_combine.set_title('ROI {}'.format(iROI), fontsize=20)\n",
    "ax_combine.set_ylabel('Fluorescence', fontsize=20)\n",
    "ax_combine.set_xlabel('Time (s)', fontsize=20)\n",
    "ax_combine.tick_params(axis = 'both', which = 'major', labelsize = 13)\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "ax_combine.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "#ax_combine.set_xlim([12.5, 14.5])\n",
    "#ax_combine.set_ylim([0, 2200])\n",
    "\n",
    "ax_combine.set_xlim([119, 126]) # reward\n",
    "ax_combine.set_xlim([68, 75]) # reward\n",
    "ax_combine.set_xlim([243, 246]) # reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
