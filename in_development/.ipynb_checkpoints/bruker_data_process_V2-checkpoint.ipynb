{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bruker raw ome-tiff preparation for preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds any .tif, .tiff, or ome.tiff files in the requested directory and first concatenates individual images into a single h5.\n",
    "\n",
    "If the user collected analog recordings from the Bruker microscope, the user can set a flag to process those analog signals and extract event times.\n",
    "\n",
    "If the user performed optogenetic stimulation (mark points), the user can set a flag to process the data and extract frame times where stimulation occurred as well as plot the mark points ROIs on the mean image.\n",
    "\n",
    "** NOTE: Raw ome-tiff images from Bruker need to be in the root directory (fdir) along with any xml meta data files.\n",
    "\n",
    "\n",
    "How to run this code\n",
    "------------------------------------\n",
    "\n",
    "__In this jupyter notebook, just run all cells in order (shift + enter). When you reach the last cell, it will prompt the user for input. You have two options:__\n",
    "\n",
    "1) __Edit the parameters in define_params to analyze a single session:__ First make edits to the parameters in the second cell of this notebook (in the define_params function). In the last line of the same cell, change the argument string argument for the method parameter to 'single'. Then run all the cells of this jupyter notebook in order (shift + enter).\n",
    "\n",
    "2) You can also indicate specific files, parameters, and processing steps to include by __editing the python script called files_to_analyze_preprocess.py__ (in the same directory as this notebook). Follow the directions in that code; briefly you should first define the general parameters that will be applied to each session, then make additional dictionaries in the 'individual_files' list for each session to be analyzed. \n",
    "a) Once you have specified the files and parameters in files_to_analyze_preprocess.py and saved the edits: In the last line of the 2nd cell of this notebook, change the argument string argument for the method parameter to 'f2a'\n",
    "b) Then execute all the cells in this notebook in order; this code will automatically load the information in files_to_analyze_preprocess.py.\n",
    "\n",
    "\n",
    "Required Packages\n",
    "-----------------\n",
    "Python 2.7, scipy, h5py, multiprocessing, matplotlib, PIL, tifffile, lxml, pandas\n",
    "\n",
    "Custom code requirements: bruker_marked_pts_process, files_to_analyze_preprocess, utils_bruker\n",
    "\n",
    "Parameters (Only relevant if using the subfunction batch_process; ignore if using files_to_analyze or using default params by inputting a file directory)\n",
    "----------\n",
    "\n",
    "fname : string\n",
    "    name the session\n",
    "\n",
    "fdir : string\n",
    "    root file directory containing the raw ome-tiff files. Note: leave off the last backslash. For example: r'C:\\Users\\my_user\\analyze_sessions'\n",
    "\n",
    "Optional Parameters (Only relevant if using batch_process)\n",
    "-------------------\n",
    "\n",
    "# parameters for stitching bruker ome-tiffs to h5/tiffstack\n",
    "flag_make_h5_tiff : boolean\n",
    "    \n",
    "save_type: 'h5',\n",
    "'number_frames': None, # optional; number of frames to analyze; defaults to analyzing whole session (None)\n",
    "\n",
    "'flag_bruker_analog': True, # set to true if analog/voltage input signals are present and are of interest\n",
    "\n",
    "'flag_bruker_stim': True, # set to true if mark points SLM stim was performed\n",
    "\n",
    "#### General analog processing variables\n",
    "analog_names : list of strings\n",
    "    strings should correspond to the TTL names for each analog channel\n",
    "                \n",
    "flag_validation_plots: boolean, # set to true if want to plot traces of ttl pulses for visualizing and validating\n",
    "                'valid_plot_channel': 'input_1', # analog dataframe column names get cleaned up; AI's are \"input_#\"\n",
    "                \n",
    "                # variables for splitting analog channels encoding multiple conditions\n",
    "                'flag_multicondition_analog': False,\n",
    "                'ai_to_split': 2, # int, analog port number that contains TTLs of multiple conditions; events here will be split into individual conditions if flag_multicondition_analog is set to true\n",
    "                'behav_id_of_interest': [101,102,103],\n",
    "\n",
    "                'flag_plot_stim_threshold': True\n",
    "\n",
    "max_disp : list of two entries\n",
    "    Each entry is an int. First entry is the y-axis maximum allowed displacement, second is the x-axis max allowed displacement.\n",
    "    The number of pixel shift for each line cannot go above these values.\n",
    "    Note: 50 pixels is approximately 10% of the FOV (512x512 pixels)\n",
    "    \n",
    "    Defaults to [30, 50]\n",
    "    \n",
    "save_displacement : bool \n",
    "    Whether or not to have SIMA save the calculated displacements over time. def: False; NOTE: if this is switched to True,\n",
    "    it can double the time to perform motion correction.\n",
    "    \n",
    "    Defaults to False\n",
    "    \n",
    "Output\n",
    "-------\n",
    "motion corrected file (in the format of h5) with \"\\_sima_mc\" appended to the end of the file name\n",
    "\n",
    "\"\\*\\_sima_masks.npy\" : numpy data file  \n",
    "  * 3D array containing 2D masks for each ROI\n",
    "\n",
    "\"\\*_extractedsignals.npy\" : numpy data file  \n",
    "  * array containing pixel-averaged activity time-series for each ROI\n",
    "   \n",
    "\"\\_spatial_weights_*.h5\" : h5 file  \n",
    "  * contains spatial weighting masks of neuropil for each ROI\n",
    "\n",
    "\"\\_neuropil_signals_*.npy\" : numpy data file  \n",
    "  * array containing neuropil signals for each ROI\n",
    "\n",
    "\"\\_neuropil_corrected_signals_*.npy\" : numpy data file  \n",
    "  * array containing neuropil-corrected signals for each ROI\n",
    "\n",
    "\"\\*.json\" : json file\n",
    "  * file containing the analysis parameters (fparams). Set by files_to_analyze.py or default parameters.\n",
    "  * to view the data, one can easily open in a text editor (eg. word or wordpad).\n",
    "\n",
    "output_images : folder containing images  \n",
    "    You will also find a folder containing plots that reflect how each executed preprocessing step performed. Examples are mean images for motion corrected data, ROI masks overlaid on mean images, extracted signals for each ROI, etc..\n",
    "\n",
    "note: * is a wildcard indicating additional characters present in the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### packages for raw video to h5 processing\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy import signal\n",
    "import h5py\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL.TiffTags import TAGS\n",
    "import tifffile as tiff\n",
    "from lxml.html.soupparser import fromstring\n",
    "from lxml.etree import tostring\n",
    "\n",
    "# custom code\n",
    "import bruker_marked_pts_process\n",
    "import files_to_analyze_preprocess\n",
    "\n",
    "#### more packages for xml meta and analog input processing\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# custom code\n",
    "import utils_bruker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "User-defined variables\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def define_params(method = 'single'):\n",
    "    \n",
    "    fparams = {}\n",
    "    \n",
    "    if method == 'single':\n",
    "        \n",
    "        fparams = [\n",
    "            {\n",
    "                # ONLY EDIT LINES BELOW THIS COMMENT (parameters for analyzing a single session)\n",
    "                \n",
    "                # parameters for stitching bruker ome-tiffs to h5/tiffstack\n",
    "                'flag_make_h5_tiff': True,\n",
    "                'fname': 'vj_ofc_imageactivate_001_2020903-001',   # \n",
    "                'fdir': r'D:\\bruker_data\\vj_ofc_imageactivate_001_20200903\\vj_ofc_imageactivate_001_2020903-001', #  \n",
    "                'save_type': 'h5',\n",
    "                'number_frames': None, # optional; number of frames to analyze; defaults to analyzing whole session (None)\n",
    "                \n",
    "                # flags for performing the main sub-analyses\n",
    "                'flag_bruker_analog': True, # set to true if analog/voltage input signals are present and are of interest\n",
    "                'flag_bruker_stim': True, # set to true if mark points SLM stim was performed\n",
    "\n",
    "                # general analog processing variables\n",
    "                'analog_names': ['stim', 'frames', 'licks', 'rewards'],\n",
    "                # variables for plotting TTL pulses\n",
    "                'flag_validation_plots': False, # set to true if want to plot traces of ttl pulses for visualizing and validating\n",
    "                'valid_plot_channel': 'input_1', # analog dataframe column names get cleaned up; AI's are \"input_#\"\n",
    "                \n",
    "                # variables for splitting analog channels encoding multiple conditions\n",
    "                'flag_multicondition_analog': False,\n",
    "                'ai_to_split': 2, # int, analog port number that contains TTLs of multiple conditions; events here will be split into individual conditions if flag_multicondition_analog is set to true\n",
    "                'behav_id_of_interest': [101,102,103],\n",
    "\n",
    "                'flag_plot_stim_threshold': True # boolean to plot the 2p pixel-avg tseries with threshold for detecting stimmed blank frames\n",
    "            \n",
    "                # ONLY EDIT LINES ABOVE THIS COMMENT\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    elif method == 'f2a': # if string is empty, load predefined list of files in files_to_analyze_event\n",
    "\n",
    "        fparams_with_general = files_to_analyze_preprocess.define_fparams()\n",
    "        \n",
    "        for this_file_dict in fparams_with_general['individual_files']:\n",
    "            this_file_dict.update(fparams_with_general['general_params']) \n",
    "        fparams = fparams_with_general['individual_files']\n",
    "        \n",
    "    elif method == 'root_dir':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    return fparams\n",
    "\n",
    "fparams = define_params(method = 'f2a') # options are 'single', 'f2a', 'root_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions for raw video conversion to h5\n",
    "\n",
    "\n",
    "def uint16_scale(img):\n",
    "    tmp = img - np.min(img) # shift values such that there are no negatives\n",
    "\n",
    "    ratio = np.amax(tmp) / 65535.0\n",
    "\n",
    "    return np.squeeze(tmp/ratio) \n",
    "\n",
    "\n",
    "def read_shape_tiff(data_path):\n",
    "    \n",
    "    data = uint16_scale(tiff.imread(data_path)).astype('uint16')\n",
    "    data_shape = data.shape\n",
    "\n",
    "    return data, data_shape\n",
    "\n",
    "def get_ometif_xy_shape(fpath):\n",
    "    # read first tiff to get data shape\n",
    "    first_tif = tiff.imread(fpath, key=0, is_ome=True)\n",
    "    return first_tif.shape\n",
    "\n",
    "\n",
    "def get_tif_meta(tif_path):\n",
    "    meta_dict = {}\n",
    "    # iterate through metadata and create dict for key/value pairs\n",
    "    with Image.open(tif_path) as img:\n",
    "        for key in img.tag.iterkeys():\n",
    "            if key in TAGS:\n",
    "                meta_dict[TAGS[key]] = img.tag[key] \n",
    "            else:\n",
    "                meta_dict[key] = img.tag[key] \n",
    "    \n",
    "    return meta_dict\n",
    "\n",
    "\n",
    "def check_if_meta_tif(path):\n",
    "    \n",
    "    meta_dict = get_tif_meta(path)\n",
    "\n",
    "    # 'ImageDescription' key contains info about the file(s) in xml format   \n",
    "    tag_soup = str(meta_dict['ImageDescription'][0][21:])\n",
    "    root_meta = fromstring(tag_soup) # process xml string\n",
    "\n",
    "    # the 'image' tag in the xml is unique to the first tif with metadata; check for that\n",
    "    subdict_image = []\n",
    "    for neighbor in root_meta.iter('image'):\n",
    "         subdict_image = neighbor.attrib\n",
    "\n",
    "    return 'id' in subdict_image\n",
    "\n",
    "\n",
    "\n",
    "def assert_bruker(fpath):\n",
    "    meta_dict = get_tif_meta(fpath)\n",
    "    assert ('Prairie' in meta_dict['Software'][0]), \"This is not a bruker file!\"\n",
    "    \n",
    "    \n",
    "def load_save_composite_frames(save_object, glob_list, chunked_frame_idx, save_format):\n",
    "    # go through each chunk, load frames in chunk, process, and append to file\n",
    "    for idx, chunk_frames in enumerate(chunked_frame_idx):\n",
    "        print( 'Processing chunk {} out of {} chunks'.format(str(idx+1), str(len(chunked_frame_idx))) )\n",
    "        start_idx = chunk_frames[0]\n",
    "        end_idx = chunk_frames[-1]+1\n",
    "\n",
    "        #loaded_tiffs = uint16_scale(tiff.imread(glob_list[start_idx:end_idx], key=0, is_ome=True))\n",
    "        data_to_save = tiff.imread(glob_list[start_idx:end_idx], key=0, is_ome=True)\n",
    "\n",
    "        if save_format == 'tif':\n",
    "\n",
    "            for frame in tiffs_to_save:\n",
    "                save_object.save(frame, photometric='minisblack')\n",
    "\n",
    "        # https://stackoverflow.com/questions/25655588/incremental-writes-to-hdf5-with-h5py\n",
    "        elif save_format == 'h5':   \n",
    "\n",
    "            # append data to h5    \n",
    "            save_object[start_idx:end_idx] = data_to_save\n",
    "\n",
    "            \n",
    "def main_ometif_to_composite(fdir, fname, save_type='h5', num_frames=None):\n",
    "\n",
    "    save_fname = os.path.join(fdir, fname)\n",
    "    glob_list = glob.glob(os.path.join(fdir,\"*.tif\"))\n",
    "\n",
    "    # get frame info\n",
    "    if not num_frames: # CZ tmp: comment back in once make this into a function\n",
    "        num_frames = len(glob_list)\n",
    "    frame_shape = get_ometif_xy_shape(glob_list[0])\n",
    "    print(str(num_frames) + ' total frame')\n",
    "\n",
    "    # prepare to split data into chunks when loading to reduce memory imprint\n",
    "    chunk_size = 10000.0\n",
    "    n_chunks = int(np.ceil(num_frames/chunk_size))\n",
    "    chunked_frame_idx = np.array_split(np.arange(num_frames), n_chunks) # split frame indices into chunks\n",
    "\n",
    "    assert_bruker(glob_list[0])\n",
    "    print('Processing Bruker data')\n",
    "\n",
    "    # prepare handles to write data to\n",
    "    if save_type == 'tif':\n",
    "        save_object = tiff.TiffWriter(save_fname + '.tif', bigtiff=True)\n",
    "    elif save_type == 'h5':\n",
    "        f = h5py.File(save_fname + '.h5', 'w')\n",
    "        # get data shape and chunk up data, and initialize h5 \n",
    "        save_object = f.create_dataset('imaging', (num_frames, frame_shape[0], frame_shape[1]), \n",
    "                                maxshape=(None, frame_shape[0], frame_shape[1]), dtype='uint16')\n",
    "        \n",
    "    load_save_composite_frames(save_object, glob_list, chunked_frame_idx, save_type)\n",
    "    \n",
    "    if save_type == 'h5':\n",
    "        f.close()\n",
    "\n",
    "### functions for meta data xml processing and analog processing\n",
    "\n",
    "# load in recording/tseries main xml and grab frame period\n",
    "def bruker_xml_get_2p_fs(xml_path):\n",
    "    xml_parse = ET.parse(xml_path).getroot()\n",
    "    for child in list(xml_parse.findall('PVStateShard')[0]):\n",
    "        if 'framePeriod' in ET.tostring(child):\n",
    "            return 1.0/float(child.attrib['value'])\n",
    "\n",
    "        \n",
    "# takes bruker xml data, parses for each frame's timing and cycle\n",
    "def bruker_xml_make_frame_info_df(xml_path):\n",
    "    xml_parse = ET.parse(xml_path).getroot()\n",
    "    frame_info_df = pd.DataFrame()\n",
    "    for idx, type_tag in enumerate(xml_parse.findall('Sequence/Frame')):\n",
    "        # extract relative and absolute time from each frame's xml meta data\n",
    "        frame_info_df.loc[idx, 'rel_time'] = float(type_tag.attrib['relativeTime'])\n",
    "        frame_info_df.loc[idx, 'abs_time'] = float(type_tag.attrib['absoluteTime'])\n",
    "\n",
    "        # grab cycle number from frame's name\n",
    "        frame_fname = type_tag.findall('File')[0].attrib['filename']\n",
    "        frame_info_df.loc[idx, 'cycle_num'] = int(re.findall('Cycle(\\d+)', frame_fname)[0])\n",
    "    return frame_info_df\n",
    "\n",
    "\n",
    "# loads and parses the analog/voltage recording's xml and grabs sampling rate\n",
    "def bruker_analog_xml_get_fs(xml_fpath):\n",
    "    analog_xml = ET.parse(xml_fpath).getroot()\n",
    "    return float(analog_xml.findall('Experiment')[0].find('Rate').text)\n",
    "\n",
    "\n",
    "# concatenate the analog input csv files if there are multiple cycles\n",
    "def bruker_concatenate_analog(fname, fpath):\n",
    "    # grab all csv voltage recording csv files that aren't the concatenated full\n",
    "    glob_analog_csv = [f for f in glob.glob(os.path.join(fpath,\"*_VoltageRecording_*.csv\")) if 'full' not in f]\n",
    "    glob_analog_xml = glob.glob(os.path.join(fpath,\"*_VoltageRecording_*.xml\"))\n",
    "\n",
    "    # xml's contain metadata about the analog csv; make sure sampling rate is consistent across cycles\n",
    "    analog_xml_fs = set(map(bruker_analog_xml_get_fs, glob_analog_xml)) # map grabs sampling rate across all cycle xmls; set finds all unique list entries  \n",
    "    if len(analog_xml_fs) > 1: \n",
    "          warnings.warn('Sampling rate is not consistent across cycles!')\n",
    "    else:\n",
    "        analog_fs = list(analog_xml_fs)[0]\n",
    "    \n",
    "    # cycle through analog csvs and append to a dataframe\n",
    "    analog_concat = pd.DataFrame()\n",
    "    for cycle_idx, cycle_path_csv in enumerate(glob_analog_csv):\n",
    "\n",
    "        cycle_df = pd.read_csv(cycle_path_csv)\n",
    "        num_samples = len(cycle_df['Time(ms)'])\n",
    "        cycle_df['Time(s)'] = cycle_df['Time(ms)']/1000.0\n",
    "\n",
    "        cycle_df['cycle_num'] = float(re.findall('Cycle(\\d+)', cycle_path_csv)[0]) # get cycle # from filename\n",
    "        if cycle_idx == 0: # initialize pd dataframe with first cycle's data\n",
    "            cycle_df['cumulative_time_ms'] = cycle_df['Time(ms)'].values\n",
    "            analog_concat = cycle_df\n",
    "        else:\n",
    "            # since time resets for each cycle (if more than one), calculate cumulative time\n",
    "            last_cumulative_time = analog_concat['cumulative_time_ms'].iloc[-1]\n",
    "            cycle_df['cumulative_time_ms'] = cycle_df['Time(ms)'].values + last_cumulative_time + 1 # add 1 so that new cycle's first sample isn't the same as the last cycle's last sample\n",
    "            analog_concat = analog_concat.append(cycle_df, ignore_index = True)\n",
    "    \n",
    "    # clean up column names\n",
    "    analog_concat.columns = analog_concat.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '_').str.replace(')', '')\n",
    "        \n",
    "    # loop through all analog columns and get the diff and threshold for event onsets\n",
    "    analog_column_names = [column for column in analog_concat.columns if 'input' in column]\n",
    "    num_analogs = len(analog_column_names)   \n",
    "    for analog_column_name in analog_column_names:\n",
    "        analog_concat[analog_column_name + '_diff'] = np.append(np.diff(analog_concat[analog_column_name]) > 0.07, \n",
    "                                                                False) # add a false to match existing df length\n",
    "\n",
    "    # save concatenated analog csv        \n",
    "    save_full_csv_path = os.path.join(fpath, fname + '_VoltageRecording_full.csv')\n",
    "    analog_concat.to_csv(save_full_csv_path, index=False)\n",
    "\n",
    "    return analog_concat\n",
    "\n",
    "\n",
    "# function for finding the index of the closest entry in an array to a provided value\n",
    "def find_nearest_idx(array, value):\n",
    "\n",
    "    if isinstance(array, pd.Series):\n",
    "        idx = (np.abs(array - value)).idxmin()\n",
    "        return idx, array.index.get_loc(idx), array[idx] # series index, 0-relative index, entry value\n",
    "    else:\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx, array[idx]\n",
    "\n",
    "\n",
    "### Take in analog dataframe (contains analog tseries and thresholded boolean) and make dict of 2p frame times for each condition's event\n",
    "def match_analog_event_to_2p(imaging_info_df, analog_dataframe, rename_ports = None, flag_multicondition_analog = False): \n",
    "\n",
    "    analog_event_dict = {} # will contain analog channel names as keys and 2p imaging frame numbers for each event/ttl onset\n",
    "    analog_event_samples = {}\n",
    "    all_diff_columns = [diff_column for diff_column in analog_dataframe.columns if 'diff' in diff_column] # grab all diff'd analog column names\n",
    "\n",
    "    for ai_diff in sorted(all_diff_columns):\n",
    "        \n",
    "        # if user gives ports to rename, grab port data name\n",
    "        if rename_ports:\n",
    "            ai_port_num = int(re.findall('\\d+', ai_diff )[0])\n",
    "            ai_name = rename_ports[ai_port_num]\n",
    "        else:\n",
    "            ai_name = ai_diff\n",
    "        \n",
    "        if flag_multicondition_analog: # if the trials in analog ports need to be split up later, make a subdict to accommodate conditions keys\n",
    "            analog_event_dict[ai_name] = {}; analog_event_dict[ai_name]['all'] = []\n",
    "            analog_event_samples[ai_name] = {}; analog_event_samples[ai_name]['all'] = []\n",
    "        else:\n",
    "            analog_event_dict[ai_name] = []\n",
    "            analog_event_samples[ai_name] = [] \n",
    "            \n",
    "        # grab analog samples where TTL onset occurred\n",
    "        # analog_df diff columns are booleans for each frame that indicate if TTL threshold crossed (ie. event occurred)\n",
    "        analog_events = analog_dataframe.loc[analog_dataframe[ai_diff] == True, ['time_s', 'cycle_num']] \n",
    "\n",
    "        # for each detected analog event, find nearest 2p frame index and add to analog event dict\n",
    "        \n",
    "        for idx, analog_event in analog_events.iterrows():\n",
    "\n",
    "            this_cycle_imaging_info = imaging_info_df[imaging_info_df['cycle_num'] == analog_event['cycle_num']]\n",
    "            \n",
    "            whole_session_idx, cycle_relative_idx, value = find_nearest_idx(this_cycle_imaging_info['rel_time'], analog_event['time_s'])\n",
    "\n",
    "            if flag_multicondition_analog:\n",
    "                analog_event_dict[ai_name]['all'].append(whole_session_idx)\n",
    "                analog_event_samples[ai_name]['all'].append(idx)\n",
    "            else:\n",
    "                analog_event_dict[ai_name].append(whole_session_idx)\n",
    "                analog_event_samples[ai_name].append(idx)\n",
    "\n",
    "    return analog_event_dict, analog_event_samples\n",
    "\n",
    "    \n",
    "# if all behav events of interest (different conditions) are recorded on a single AI channel\n",
    "# and need to reference the behavioral events csv to split conditions up\n",
    "def split_analog_channel(ai_to_split, fdir, behav_fname, behav_event_key_path, analog_event_dict):\n",
    "\n",
    "    unicode_to_str = lambda x:str(x) # just a simple function to convert unicode to string; \n",
    "\n",
    "    this_ai_to_split = [analog_diff_name for analog_diff_name in analog_event_dict.keys() if str(ai_to_split) in analog_diff_name][0]\n",
    "    \n",
    "    # load id's and samples (camera samples?) of behavioral events (output by behavioral program)\n",
    "    behav_df = pd.read_csv(os.path.join(fdir, behav_fname), names=['id', 'sample'])\n",
    "    behav_event_keys = pd.read_excel(behav_event_key_path)\n",
    "\n",
    "    # using the behav event id, grab the event name from the keys dataframe; names are in unicode, so have to convert to string\n",
    "    behav_name_of_interest = map(unicode_to_str, \n",
    "                                 behav_event_keys[behav_event_keys['event_id'].isin(behav_id_of_interest)]['event_desc'].values)\n",
    "\n",
    "    # go into ordered behavioral event df, grab the trials with condition IDs of 'behav_id_of_interest' in order\n",
    "    trial_ids = list(behav_df[behav_df['id'].isin(behav_id_of_interest)]['id'].values) # grab 101, 102, 103 trials in order\n",
    "    \n",
    "    # loop through behav conditions, and separate event times for the conglomerate event times in analog_event_dict\n",
    "    for behav_event_id, behav_event_name in zip(behav_id_of_interest, behav_name_of_interest):\n",
    "        this_event_idxs = [idx for idx,val in enumerate(trial_ids) if val==behav_event_id]\n",
    "        analog_event_dict[this_ai_to_split][behav_event_name] = [analog_event_dict[this_ai_to_split]['all'][idx] for idx in this_event_idxs]\n",
    "        # analog_event_dict ultimately contains 2p frame indices for each event categorized by condition\n",
    "\n",
    "    # save preprocessed behavioral event data\n",
    "    with open(behav_analog_save_path, 'wb') as handle:\n",
    "        pickle.dump(analog_event_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# take in data from an analog input and plot detected ttls\n",
    "def plot_analog_validation(AI_onsets, analog_tseries, analog_fs, save_dir = None):\n",
    "    # following is just for visualizing ttls; here make tiles for indexing and extracting ttl data in trial manner\n",
    "    num_AI = len(AI_onsets)\n",
    "    rel_ind_vec = np.arange(-0.5*analog_fs, 3*analog_fs, 1)\n",
    "    rel_ind_tile = np.tile(rel_ind_vec, (num_AI,1))\n",
    "    AI_onset_tile = np.tile(AI_onsets, (len(rel_ind_vec),1)).T\n",
    "\n",
    "    # extract analog values across flattened trial indices, get values of series, then reshape to 2d array\n",
    "    AI_value_tile = analog_tseries[np.ndarray.flatten(AI_onset_tile + rel_ind_tile)].values.reshape(AI_onset_tile.shape)\n",
    "    if AI_value_tile.shape[0] == num_AI:\n",
    "        AI_value_tile = AI_value_tile.T\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3,figsize=(17,5))\n",
    "\n",
    "    ax[0].set_title('Full TTL series')\n",
    "    ax[0].plot(analog_tseries)\n",
    "\n",
    "    ax[1].set_title('{} ttls detected'.format(num_AI))\n",
    "    ax[1].plot( AI_value_tile );\n",
    "    ax[1].set_xlabel('Time (ms)')\n",
    "    ax[1].set_ylabel('Volts');\n",
    "\n",
    "    svec = np.arange(0, 15*analog_fs)\n",
    "    tvec_plot = svec/analog_fs\n",
    "    ax[2].set_title('Specific window (first 15s)')\n",
    "    ax[2].plot(tvec_plot, analog_tseries[svec])\n",
    "    ax[2].set_xlabel('Seconds')\n",
    "    \n",
    "    if save_path:\n",
    "        utils.check_exist_dir(save_dir)\n",
    "        fig.savefig(os.path.join(save_dir, 'ttl_validation.png'));\n",
    "\n",
    "        \n",
    "def make_imaging_info_df(bruker_tseries_xml_path):\n",
    "    xml_parse = ET.parse(bruker_tseries_xml_path).getroot()\n",
    "    frame_info_df = pd.DataFrame()\n",
    "    type_tags = xml_parse.findall('Sequence/Frame')\n",
    "\n",
    "    # lambda function to take in a list of xml frame meta data and pull out timing and cycle info \n",
    "    grab_2p_xml_frame_time = lambda type_tag: [float(type_tag.attrib['relativeTime']), \n",
    "                                               float(type_tag.attrib['absoluteTime']),\n",
    "                                               int(re.findall('Cycle(\\d+)', type_tag.findall('File')[0].attrib['filename'])[0]) # first grab this frame's file name, then use regex to grab cycle number in the fname\n",
    "                                              ] \n",
    "\n",
    "    # make a dataframe of relative time, absolute time, cycle number for each frame\n",
    "    imaging_info_df = pd.DataFrame(map(grab_2p_xml_frame_time, type_tags), columns=['rel_time', 'abs_time', 'cycle_num'])\n",
    "\n",
    "    return imaging_info_df\n",
    "\n",
    "\n",
    "# make a dict of possible paths for loading and saving\n",
    "def bruker_analog_define_paths(fdir, fname):\n",
    "    paths_dict = {}\n",
    "    paths_dict['bruker_tseries_xml_path'] = os.path.join(fdir, fname + '.xml') # recording/tseries main xml\n",
    "    paths_dict['glob_analog_csv'] = glob.glob(os.path.join(fdir,\"*_VoltageRecording_*.csv\")) # grab all analog/voltage recording csvs\n",
    "    paths_dict['glob_analog_xml'] = glob.glob(os.path.join(fdir,\"*_VoltageRecording_*.xml\")) # grab all analog/voltage recording xml meta\n",
    "    # behavioral event identification files\n",
    "    paths_dict['behav_fname'] = fname + '_taste_reactivity.csv' # csv containing each behav event and corresponding sample\n",
    "    paths_dict['behav_event_key_path'] = r'D:\\bruker_data\\Adam\\key_event.xlsx' # location of excel matching event names and id's\n",
    "    # define save paths\n",
    "    paths_dict['behav_save_path'] = os.path.join(fdir, 'framenumberforevents_{}.pkl'.format(fname) )\n",
    "    paths_dict['behav_analog_save_path'] = os.path.join(fdir, 'framenumberforevents_analog_{}.pkl'.format(fname) )\n",
    "     \n",
    "    return paths_dict\n",
    "\n",
    "# load 2p recording meta xml and extract the info into a dict\n",
    "def bruker_make_2p_meta_dict(fdir, fname, paths_dict):\n",
    "    meta_2p_dict = {}\n",
    "    meta_2p_dict['fs_2p'] = bruker_xml_get_2p_fs(paths_dict['bruker_tseries_xml_path'])\n",
    "    # extract frame timing and cycle info into a pandas dataframe \n",
    "    meta_2p_dict['imaging_info_df'] = make_imaging_info_df(paths_dict['bruker_tseries_xml_path'])\n",
    "    # Parse main 2p time-series xml\n",
    "    meta_2p_dict['tvec_2p'] = meta_2p_dict['imaging_info_df']['rel_time']\n",
    "    meta_2p_dict['num_frames_2p'] = len(meta_2p_dict['tvec_2p'])\n",
    "\n",
    "    return meta_2p_dict\n",
    "\n",
    "\n",
    "# see description below: performs main lifting of analog data processing\n",
    "def bruker_process_analog_ttl(fparams, paths_dict, meta_2p_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    If you have analog signals, that indicate behavioral event onset, sent from your behavioral DAQ to the bruker GPIO box, the following code:\n",
    "\n",
    "    1) parses the analog voltage recording xmls \n",
    "    2) extracts the signals from the csvs\n",
    "    3) extracts the TTL onset times\n",
    "    4) and finally lines up which frame the TTL occurred on.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### get analog data sampling rate from xml\n",
    "    analog_fs = bruker_analog_xml_get_fs(paths_dict['glob_analog_xml'][0])\n",
    "\n",
    "    ### either load concatenated voltage recording (across cycles), perform the concatenation, or load a single CSV (for single cycle)\n",
    "    volt_rec_full_path = os.path.join(fparams['fdir'], fparams['fname'] + '_VoltageRecording_full.csv')\n",
    "    if os.path.exists(volt_rec_full_path): # if a trial-stitched voltage recording was previously saved\n",
    "        analog_df = pd.read_csv(volt_rec_full_path)\n",
    "    else:\n",
    "        analog_df = bruker_concatenate_analog(fparams['fname'], fparams['fdir']) \n",
    "\n",
    "    ### match analog ttl event onsets to the corresponding 2p frame (for each event in each analog port)\n",
    "    analog_event_dict, analog_event_samples = match_analog_event_to_2p(meta_2p_dict['imaging_info_df'], \n",
    "                                                                       analog_df, rename_ports = fparams['analog_names'])\n",
    "\n",
    "    ### if there are multiple conditions signaled on a single analog port, split them up, resave as pickle\n",
    "    if fparams['flag_multicondition_analog']:\n",
    "        split_analog_channel(fparams['ai_to_split'], fparams['fdir'], \n",
    "                             paths_dict['behav_fname'], paths_dict['behav_event_key_path'], analog_event_dict) \n",
    "\n",
    "    if fparams['flag_validation_plots']:\n",
    "        valid_save_dir = os.path.join(fparams['fdir'], fparams['fname'] + '_output_images')\n",
    "        utils_bruker.check_exist_dir(valid_save_dir)\n",
    "        plot_analog_validation(analog_event_samples, analog_df[fparams['valid_plot_channel']], \n",
    "                               analog_fs, valid_save_dir);\n",
    "\n",
    "    # save preprocessed behavioral event data\n",
    "    with open(paths_dict['behav_save_path'], 'wb') as handle:\n",
    "        pickle.dump(analog_event_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "# uses bruker xmls and analog voltage_recordings to identify event/TTL frames/times\n",
    "def main_bruker_analog(fparam):\n",
    "    # define file paths and output file names\n",
    "    paths_dict = bruker_analog_define_paths(fparam['fdir'], fparam['fname'])\n",
    "\n",
    "    # get more timing meta data about 2p from xmls\n",
    "    meta_2p_dict = bruker_make_2p_meta_dict(fparam['fdir'], fparam['fname'], paths_dict)\n",
    "\n",
    "    \"\"\"\n",
    "    If you have analog signals, that indicate behavioral event onset, sent from your behavioral DAQ to the bruker GPIO box, the following code:\n",
    "\n",
    "    1) parses the analog voltage recording xmls \n",
    "    2) extracts the signals from the csvs\n",
    "    3) extracts the TTL onset times\n",
    "    4) and finally lines up which frame the TTL occurred on.\n",
    "    \"\"\"\n",
    "    if fparam['flag_bruker_analog']: \n",
    "        print('Processing analog TTLs; outputs framenumberforevents pickle file')\n",
    "        bruker_process_analog_ttl(fparam, paths_dict, meta_2p_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_file_process(fparam):\n",
    "    \n",
    "    if fparam['flag_make_h5_tiff']:\n",
    "        main_ometif_to_composite(fparam['fdir'], fparam['fname'], fparam['save_type'], num_frames=fparam['number_frames'])\n",
    "    \n",
    "    # Meta, Analog/TTL, & Behavioral Data Preprocessing\n",
    "    if fparam['flag_bruker_analog']:\n",
    "        main_bruker_analog(fparam)\n",
    "\n",
    "    if fparam['flag_bruker_stim']:\n",
    "        print('Detecting stimulation times/frames; outputs _stimmed_frames.pkl file')\n",
    "        bruker_marked_pts_process.main_detect_save_stim_frames(fparam['fdir'], fparam['fname'], \n",
    "                                                               detection_threshold=1.5, flag_plot_mk_pts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_files = len(fparams)\n",
    "if num_files == 0:\n",
    "    raise Exception(\"No files to analyze!\")\n",
    "print(str(num_files) + ' files to analyze')\n",
    "\n",
    "# determine number of cores to use and initialize parallel pool\n",
    "num_processes = min(mp.cpu_count(), num_files)\n",
    "print('Total CPU cores for parallel processing: ' + str(num_processes))\n",
    "pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "# perform parallel processing; pass iterable list of file params to the analysis module selection code\n",
    "#pool.map(single_file_process, fparams)\n",
    "\n",
    "## for testing\n",
    "for fparam in fparams:\n",
    "    single_file_process(fparam) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
